{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"banpu-interns-data-exchange\"\n",
    "file_path = \"h5/test_8.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Use AWS CLI named profile\n",
    "session = boto3.Session(profile_name=\"bivlab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload model file to S3\n",
    "s3 = session.client(\"s3\")\n",
    "s3.upload_file(file_path, bucket_name, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Create SageMaker execution role\n",
    "iam = session.client(\"iam\")\n",
    "role_name = \"SageMakerRole\"\n",
    "\n",
    "assume_role_policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"sagemaker.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    create_role_response = iam.create_role(\n",
    "        RoleName=role_name,\n",
    "        Description=\"Allows SageMaker to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=json.dumps(assume_role_policy_document)\n",
    "    )\n",
    "\n",
    "    role_arn = create_role_response[\"Role\"][\"Arn\"]\n",
    "except iam.exceptions.EntityAlreadyExistsException:\n",
    "    role_arn = iam.get_role(RoleName=role_name)[\"Role\"][\"Arn\"]\n",
    "\n",
    "# Attach AmazonSageMakerFullAccess policy to the role\n",
    "policy_arn = \"arn:aws:iam::aws:policy/AmazonSageMakerFullAccess\"\n",
    "\n",
    "try:\n",
    "    iam.attach_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyArn=policy_arn\n",
    "    )\n",
    "except iam.exceptions.PolicyAlreadyExistsException:\n",
    "    pass\n",
    "\n",
    "# Attach AmazonS3FullAccess policy to the role\n",
    "policy_arn = \"arn:aws:iam::aws:policy/AmazonS3FullAccess\"\n",
    "\n",
    "try:\n",
    "    iam.attach_role_policy(RoleName=role_name, PolicyArn=policy_arn)\n",
    "except iam.exceptions.PolicyAlreadyExistsException:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ailama/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "\n",
    "sagemaker_session = Session(boto_session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------*"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error hosting endpoint tensorflow-inference-2024-07-07-21-16-57-580: Failed. Reason: Received server error (0) from model with message \"An error occurred while handling request as the model process exited.\". See https://ap-southeast-1.console.aws.amazon.com/cloudwatch/home?region=ap-southeast-1#logEventViewer:group=/aws/sagemaker/Endpoints/tensorflow-inference-2024-07-07-21-16-57-580 in account 625531495787 for more information.. Try changing the instance type or reference the troubleshooting page https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference-troubleshooting.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m TensorFlowModel(\n\u001b[1;32m      6\u001b[0m     role\u001b[38;5;241m=\u001b[39mrole_arn,\n\u001b[1;32m      7\u001b[0m     model_data\u001b[38;5;241m=\u001b[39mmodel_data,\n\u001b[1;32m      8\u001b[0m     framework_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.14\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     sagemaker_session\u001b[38;5;241m=\u001b[39msagemaker_session,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m serverless_config \u001b[38;5;241m=\u001b[39m sagemaker\u001b[38;5;241m.\u001b[39mserverless\u001b[38;5;241m.\u001b[39mServerlessInferenceConfig(\n\u001b[1;32m     13\u001b[0m     memory_size_in_mb\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m,  \u001b[38;5;66;03m# Adjust based on model's needs\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     max_concurrency\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     15\u001b[0m )\n\u001b[0;32m---> 17\u001b[0m predictor \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdeploy(serverless_inference_config\u001b[38;5;241m=\u001b[39mserverless_config)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sagemaker/tensorflow/model.py:364\u001b[0m, in \u001b[0;36mTensorFlowModel.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe TensorFlow version \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt support EIA.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework_version\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(msg)\n\u001b[0;32m--> 364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(TensorFlowModel, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mdeploy(\n\u001b[1;32m    365\u001b[0m     initial_instance_count\u001b[38;5;241m=\u001b[39minitial_instance_count,\n\u001b[1;32m    366\u001b[0m     instance_type\u001b[38;5;241m=\u001b[39minstance_type,\n\u001b[1;32m    367\u001b[0m     serializer\u001b[38;5;241m=\u001b[39mserializer,\n\u001b[1;32m    368\u001b[0m     deserializer\u001b[38;5;241m=\u001b[39mdeserializer,\n\u001b[1;32m    369\u001b[0m     accelerator_type\u001b[38;5;241m=\u001b[39maccelerator_type,\n\u001b[1;32m    370\u001b[0m     endpoint_name\u001b[38;5;241m=\u001b[39mendpoint_name,\n\u001b[1;32m    371\u001b[0m     tags\u001b[38;5;241m=\u001b[39mformat_tags(tags),\n\u001b[1;32m    372\u001b[0m     kms_key\u001b[38;5;241m=\u001b[39mkms_key,\n\u001b[1;32m    373\u001b[0m     wait\u001b[38;5;241m=\u001b[39mwait,\n\u001b[1;32m    374\u001b[0m     data_capture_config\u001b[38;5;241m=\u001b[39mdata_capture_config,\n\u001b[1;32m    375\u001b[0m     async_inference_config\u001b[38;5;241m=\u001b[39masync_inference_config,\n\u001b[1;32m    376\u001b[0m     serverless_inference_config\u001b[38;5;241m=\u001b[39mserverless_inference_config,\n\u001b[1;32m    377\u001b[0m     volume_size\u001b[38;5;241m=\u001b[39mvolume_size,\n\u001b[1;32m    378\u001b[0m     model_data_download_timeout\u001b[38;5;241m=\u001b[39mmodel_data_download_timeout,\n\u001b[1;32m    379\u001b[0m     container_startup_health_check_timeout\u001b[38;5;241m=\u001b[39mcontainer_startup_health_check_timeout,\n\u001b[1;32m    380\u001b[0m     inference_recommendation_id\u001b[38;5;241m=\u001b[39minference_recommendation_id,\n\u001b[1;32m    381\u001b[0m     explainer_config\u001b[38;5;241m=\u001b[39mexplainer_config,\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    383\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sagemaker/model.py:1721\u001b[0m, in \u001b[0;36mModel.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, accept_eula, endpoint_logging, resources, endpoint_type, managed_instance_scaling, inference_component_name, routing_config, **kwargs)\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_explainer_enabled:\n\u001b[1;32m   1719\u001b[0m     explainer_config_dict \u001b[38;5;241m=\u001b[39m explainer_config\u001b[38;5;241m.\u001b[39m_to_request_dict()\n\u001b[0;32m-> 1721\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mendpoint_from_production_variants(\n\u001b[1;32m   1722\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint_name,\n\u001b[1;32m   1723\u001b[0m     production_variants\u001b[38;5;241m=\u001b[39m[production_variant],\n\u001b[1;32m   1724\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m   1725\u001b[0m     kms_key\u001b[38;5;241m=\u001b[39mkms_key,\n\u001b[1;32m   1726\u001b[0m     wait\u001b[38;5;241m=\u001b[39mwait,\n\u001b[1;32m   1727\u001b[0m     data_capture_config_dict\u001b[38;5;241m=\u001b[39mdata_capture_config_dict,\n\u001b[1;32m   1728\u001b[0m     explainer_config_dict\u001b[38;5;241m=\u001b[39mexplainer_config_dict,\n\u001b[1;32m   1729\u001b[0m     async_inference_config_dict\u001b[38;5;241m=\u001b[39masync_inference_config_dict,\n\u001b[1;32m   1730\u001b[0m     live_logging\u001b[38;5;241m=\u001b[39mendpoint_logging,\n\u001b[1;32m   1731\u001b[0m )\n\u001b[1;32m   1733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor_cls:\n\u001b[1;32m   1734\u001b[0m     predictor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor_cls(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sagemaker/session.py:5711\u001b[0m, in \u001b[0;36mSession.endpoint_from_production_variants\u001b[0;34m(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict, async_inference_config_dict, explainer_config_dict, live_logging, vpc_config, enable_network_isolation, role)\u001b[0m\n\u001b[1;32m   5708\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating endpoint-config with name \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, name)\n\u001b[1;32m   5709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_client\u001b[38;5;241m.\u001b[39mcreate_endpoint_config(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_options)\n\u001b[0;32m-> 5711\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_endpoint(\n\u001b[1;32m   5712\u001b[0m     endpoint_name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m   5713\u001b[0m     config_name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m   5714\u001b[0m     tags\u001b[38;5;241m=\u001b[39mendpoint_tags,\n\u001b[1;32m   5715\u001b[0m     wait\u001b[38;5;241m=\u001b[39mwait,\n\u001b[1;32m   5716\u001b[0m     live_logging\u001b[38;5;241m=\u001b[39mlive_logging,\n\u001b[1;32m   5717\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sagemaker/session.py:4569\u001b[0m, in \u001b[0;36mSession.create_endpoint\u001b[0;34m(self, endpoint_name, config_name, tags, wait, live_logging)\u001b[0m\n\u001b[1;32m   4566\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint_arn \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEndpointArn\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   4568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 4569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait_for_endpoint(endpoint_name, live_logging\u001b[38;5;241m=\u001b[39mlive_logging)\n\u001b[1;32m   4570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m endpoint_name\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sagemaker/session.py:5354\u001b[0m, in \u001b[0;36mSession.wait_for_endpoint\u001b[0;34m(self, endpoint, poll, live_logging)\u001b[0m\n\u001b[1;32m   5348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[1;32m   5349\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[1;32m   5350\u001b[0m             message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   5351\u001b[0m             allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInService\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   5352\u001b[0m             actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   5353\u001b[0m         )\n\u001b[0;32m-> 5354\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   5355\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   5356\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInService\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   5357\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   5358\u001b[0m     )\n\u001b[1;32m   5359\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m desc\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error hosting endpoint tensorflow-inference-2024-07-07-21-16-57-580: Failed. Reason: Received server error (0) from model with message \"An error occurred while handling request as the model process exited.\". See https://ap-southeast-1.console.aws.amazon.com/cloudwatch/home?region=ap-southeast-1#logEventViewer:group=/aws/sagemaker/Endpoints/tensorflow-inference-2024-07-07-21-16-57-580 in account 625531495787 for more information.. Try changing the instance type or reference the troubleshooting page https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference-troubleshooting.html"
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow.serving import TensorFlowModel\n",
    "\n",
    "model_data = \"s3://\" + bucket_name + \"/\" + file_path\n",
    "\n",
    "model = TensorFlowModel(\n",
    "    role=role_arn,\n",
    "    model_data=model_data,\n",
    "    framework_version=\"2.14\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "serverless_config = sagemaker.serverless.ServerlessInferenceConfig(\n",
    "    memory_size_in_mb=2048,  # Adjust based on model's needs\n",
    "    max_concurrency=10,\n",
    ")\n",
    "\n",
    "predictor = model.deploy(serverless_inference_config=serverless_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
